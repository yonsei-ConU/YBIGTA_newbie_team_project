from review_analysis.crawling.base_crawler import BaseCrawler
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import os
import time

class RottenTomatoesCrawler(BaseCrawler):
    """
    Seleniumì„ ì´ìš©í•˜ì—¬ Rotten Tomatoes ìœ ì € ë¦¬ë·°ë¥¼ 500ê°œ ì´ìƒ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤.
    ì˜í™”ì˜ ìœ ì € ë¦¬ë·°ì—ì„œ í‰ì (score), ë‚ ì§œ(date), ë¦¬ë·° ë‚´ìš©(review)ì„ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥í•¨.
    """

    def __init__(self, output_dir: str):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” í•¨ìˆ˜. ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° í¬ë¡¤ë§ ëŒ€ìƒ URL ì„¤ì •.
        """
        super().__init__(output_dir)
        self.url = "https://www.rottentomatoes.com/m/mickey_17/reviews?type=user"
        self.driver = None
        self.reviews: list[tuple[str, str, str]] = []

    def start_browser(self):
        """
        Chrome WebDriverë¥¼ ì‹¤í–‰í•˜ì—¬ ëŒ€ìƒ í˜ì´ì§€ ì ‘ì†
        """
        options = webdriver.ChromeOptions()
        # options.add_argument("--headless")  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        self.driver.get(self.url)

    def click_show_more(self, max_clicks=30):
        """
        "Load More" ë²„íŠ¼ì„ ìµœëŒ€ max_clicksíšŒ í´ë¦­í•˜ì—¬ ë¦¬ë·°ë¥¼ ì¶”ê°€ ë¡œë”©í•¨.
        """
        print("\U0001f501 'Load More' ë²„íŠ¼ í´ë¦­ ì¤‘...")
        for i in range(max_clicks):
            try:
                WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "rt-button[data-qa='load-more-btn']"))
                )
                self.driver.execute_script("""
                    const btn = document.querySelector("rt-button[data-qa='load-more-btn']");
                    if (btn) btn.click();
                """)
                print(f"ğŸ‘‰ {i + 1}íšŒ í´ë¦­")
                time.sleep(1.5)
            except Exception:
                print("\U0001f6d1 ë” ì´ìƒ ë²„íŠ¼ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨")
                break

    def scrape_reviews(self):
        """
        ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•˜ê³  ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ë©° ë¦¬ë·° ëª©ë¡ì„ self.reviewsì— ì €ì¥
        """
        print("\U0001f345 Rotten Tomatoes ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘")
        self.start_browser()
        self.click_show_more(max_clicks=30)

        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        review_blocks = soup.select("div.audience-review-row")

        for block in review_blocks:
            try:
                score_tag = block.select_one("rating-stars-group")
                score = score_tag.get("score", "") if score_tag else ""

                date_tag = block.select_one("span.audience-reviews__duration")
                date = date_tag.text.strip() if date_tag else ""

                review_tag = block.select_one("p.audience-reviews__review")
                review = review_tag.text.strip() if review_tag else ""

                if review:
                    self.reviews.append((score, date, review))
            except Exception as e:
                print("âš ï¸ ë¦¬ë·° íŒŒì‹± ì˜¤ë¥˜:", e)

        self.driver.quit()
        print(f"âœ… ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(self.reviews)}ê°œ")

    def save_to_database(self):
        """
        ìˆ˜ì§‘ëœ ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•¨.
        """
        df = pd.DataFrame(self.reviews, columns=["score", "date", "review"])
        os.makedirs(self.output_dir, exist_ok=True)
        save_path = os.path.join(self.output_dir, "reviews_rotten.csv")
        df.to_csv(save_path, index=False)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")

    def crawl(self):
        """
        ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰: ë¦¬ë·° ìˆ˜ì§‘ â†’ ì €ì¥
        """
        self.scrape_reviews()
        self.save_to_database()