import os
import time
import csv
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from review_analysis.crawling.base_crawler import BaseCrawler

class NaverMovieCrawler(BaseCrawler):
    """
    ë„¤ì´ë²„ ì˜í™” 'ë¯¸í‚¤ 17'ì˜ ê´€ëŒí‰ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤.

    Attributes:
        url (str): í¬ë¡¤ë§ ëŒ€ìƒ URL.
        reviews (list): í¬ë¡¤ë§í•œ ë¦¬ë·° ë°ì´í„° (date, score, text) ë¦¬ìŠ¤íŠ¸.
    """
    def __init__(self, output_dir):
"""
        NaverMovieCrawler ê°ì²´ë¥¼ ì´ˆê¸°í™”í•œë‹¤.

        Args:
            output_dir (str): í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬ ê²½ë¡œ.
        """
        super().__init__(output_dir)
        self.url = "https://search.naver.com/search.naver?where=nexearch&sm=tab_etc&mra=bkEw&pkid=68&os=29816634&qvt=0&query=ë¯¸í‚¤%2017%20ê´€ëŒí‰"
        self.reviews = []

    def start_browser(self):
        """
        ì…€ë ˆë‹ˆì›€ WebDriver(Chrome)ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
        """
        options = Options()
        # options.add_argument("--headless")  # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    def scroll_down(self, min_reviews=500):
        """
        ë™ì  ë¡œë”© ë°©ì‹ì˜ ë¦¬ë·° í˜ì´ì§€ë¥¼ ì§€ì •í•œ ê°œìˆ˜ë§Œí¼ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤.

        Args:
            min_reviews (int): ìµœì†Œ í¬ë¡¤ë§í•  ë¦¬ë·° ê°œìˆ˜.
        """
        print("ğŸ”½ ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©° ë¦¬ë·°ë¥¼ ìˆ˜ì§‘ ì¤‘...")
        scroll_area = WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.lego_review_list._scroller"))
        )

        prev_count = 0
        same_count_times = 0

        while True:
            self.driver.execute_script("arguments[0].scrollTo(0, arguments[0].scrollHeight);", scroll_area)
            time.sleep(1.5)
            current_count = len(self.driver.find_elements(By.CSS_SELECTOR, "li.area_card._item"))

            if current_count >= min_reviews:
                break
            if current_count == prev_count:
                same_count_times += 1
                if same_count_times >= 3:
                    print("âš ï¸ ë” ì´ìƒ ë¡œë“œë˜ì§€ ì•Šì•„ ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
                    break
            else:
                same_count_times = 0

            prev_count = current_count

    def parse_reviews(self):
        """
        í˜„ì¬ í˜ì´ì§€ì—ì„œ HTMLì„ íŒŒì‹±í•˜ì—¬ ë¦¬ë·° ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.

        Returns:
            list: ì¶”ì¶œëœ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸. ê° ë¦¬ë·°ëŠ” (date, score, text) í˜•íƒœì˜ íŠœí”Œ.
        """
        html = self.driver.page_source
        soup = BeautifulSoup(html, "html.parser")
        blocks = soup.select("li.area_card._item")

        reviews = []
        for block in blocks:
            score_tag = block.select_one(".area_text_box")
            review_tag = block.select_one("span.desc._text")
            date_tag = block.select_one("dd.this_text_normal")

            score = score_tag.get_text(strip=True) if score_tag else ""
            review = review_tag.get_text(strip=True) if review_tag else ""
            date = date_tag.get_text(strip=True).split()[0].replace(".", "-") if date_tag else ""

            if score and review and date:
                reviews.append((date, score, review))
        return reviews

    def scrape_reviews(self, tab_name="ê³µê°ìˆœ"):
        """
        ì§€ì •ëœ íƒ­(ê³µê°ìˆœ ë˜ëŠ” ìµœì‹ ìˆœ)ì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•œë‹¤.

        Args:
            tab_name (str): í¬ë¡¤ë§í•  íƒ­ì˜ ì´ë¦„ ("ê³µê°ìˆœ" ë˜ëŠ” "ìµœì‹ ìˆœ").

        Returns:
            list: í•´ë‹¹ íƒ­ì—ì„œ ìˆ˜ì§‘í•œ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸.
        """
        self.driver.get(self.url)
        time.sleep(2)
        if tab_name == "ìµœì‹ ìˆœ":
            try:
                latest_tab = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, '//span[text()="ìµœì‹ ìˆœ"]'))
                )
                latest_tab.click()
                print("ğŸ†• 'ìµœì‹ ìˆœ' íƒ­ í´ë¦­ ì™„ë£Œ")
                time.sleep(2)
            except Exception as e:
                print(f"âš ï¸ 'ìµœì‹ ìˆœ' íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")

        self.scroll_down(min_reviews=500)
        html_path = os.path.join(self.output_dir, f"naver_raw_{tab_name}.html")
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(self.driver.page_source)
        print(f"âœ… HTML ì €ì¥ ì™„ë£Œ: {html_path}")

        return self.parse_reviews()

    def save_to_database(self, reviews):
        """
        ìˆ˜ì§‘í•œ ë¦¬ë·°ë¥¼ ì¤‘ë³µ ì œê±° í›„ CSV íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.

        Args:
            reviews (list): (date, score, text) í˜•íƒœì˜ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸.
        """
        deduplicated = list({(d, s, r) for d, s, r in reviews})
        output_path = os.path.join(self.output_dir, "reviews_naver.csv")
        with open(output_path, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["date", "rating", "text"])
            writer.writerows(deduplicated)
        print(f"âœ… ì¤‘ë³µ ì œê±° í›„ {len(deduplicated)}ê°œ ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {output_path}")

    def crawl(self):
        """
        ì „ì²´ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•œë‹¤.
        ê³µê°ìˆœê³¼ ìµœì‹ ìˆœ íƒ­ì—ì„œ ë¦¬ë·°ë¥¼ ê°ê° ìˆ˜ì§‘í•˜ê³ , ì¤‘ë³µì„ ì œê±°í•œ ë’¤ ì €ì¥í•œë‹¤.
        """
        self.start_browser()
        try:
            sympathy = self.scrape_reviews("ê³µê°ìˆœ")
            latest = self.scrape_reviews("ìµœì‹ ìˆœ")
            combined = sympathy + latest
            self.save_to_database(combined)
        finally:
            self.driver.quit()
            print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")